---
layout: post
title: Blog Post 6 - Fake News Classification
---
In my last blog post of 16B, I will be showing how to classify fake news using Tensorflow.

## §1. Acquire Training Data


```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

# requires update to tensorflow 2.4
# >>> conda activate PIC16B
# >>> pip install tensorflow==2.4
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
stop = stopwords.words('english')


```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!



```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
data_ = pd.read_csv(train_url)
data_ = data_.drop('Unnamed: 0', axis = 1)
```


```python
data_.head()
```





  <div id="df-73ae18e9-9c45-4d19-b43d-822b51940867">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-73ae18e9-9c45-4d19-b43d-822b51940867')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-73ae18e9-9c45-4d19-b43d-822b51940867 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-73ae18e9-9c45-4d19-b43d-822b51940867');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




## §2. Make a Dataset

This will ensure any dataframe we have will be converted to a tensor slice that will make it easy to put into our model to train.


```python
def make_dataset(fromData):

  yump = fromData
  # Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply
  yump['title'] = yump['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  yump['text'] = yump['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  _data_ = tf.data.Dataset.from_tensor_slices(
    (
      {
          "title" : yump[["title"]], 
          "text" : yump[["text"]]
      }, 
      {
          "fake" : yump[["fake"]]
      }
    )
  )

  return _data_
```


```python
data = make_dataset(data_)
data
```




    <TensorSliceDataset element_spec=({'title': TensorSpec(shape=(1,), dtype=tf.string, name=None), 'text': TensorSpec(shape=(1,), dtype=tf.string, name=None)}, {'fake': TensorSpec(shape=(1,), dtype=tf.int64, name=None)})>



### Validation Data

Now we can take 60% of our data to use as training data, 20% as testing data, and 20% as validation data.


```python
data = data.batch(100)
data = data.shuffle(buffer_size = len(data))

train_size = int(0.6*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size)
val   = data.skip(train_size).take(val_size)
test  = data.skip(train_size + val_size)

len(train), len(val), len(test)
```




    (135, 45, 45)



## Base Rate


```python
fake_baseline_counts = data_.fake.value_counts()
real_news = fake_baseline_counts[0]
fake_news = fake_baseline_counts[1]

fake_baseline = fake_news / (real_news + fake_news)
fake_baseline

```




    0.522963160942581



Looks like our baseline will be fake! It is the one that happens most often at 52% of the time.

## §3. Create Models


```python
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=100) 

title = train.map(lambda x, y: x["title"][0,0])
text = train.map(lambda x, y: x["text"][0,0])
```


```python
vectorize_layer.adapt(title)
vectorize_layer.adapt(text)
```

### Model 1


```python
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)
```


```python
title_features = vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, 3, name = "titleembedding")(title_features)
title_features = layers.Dropout(0.5)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.5)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
```


```python
main = layers.Dense(32, activation='relu')(title_features)
output = layers.Dense(2, name = "fake")(main)
```


```python
model1 = keras.Model(
    inputs = title_input,
    outputs = output
)
```


```python
model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model1.fit(train, 
                    validation_data=val,
                    epochs = 20)
```

    Epoch 1/20


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning:
    
    Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
    


    135/135 [==============================] - 2s 7ms/step - loss: 0.6831 - accuracy: 0.5779 - val_loss: 0.6482 - val_accuracy: 0.7460
    Epoch 2/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.5175 - accuracy: 0.7730 - val_loss: 0.2887 - val_accuracy: 0.9267
    Epoch 3/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.3177 - accuracy: 0.8509 - val_loss: 0.1553 - val_accuracy: 0.9629
    Epoch 4/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2775 - accuracy: 0.8630 - val_loss: 0.1268 - val_accuracy: 0.9602
    Epoch 5/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2392 - accuracy: 0.8819 - val_loss: 0.1060 - val_accuracy: 0.9747
    Epoch 6/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2396 - accuracy: 0.8832 - val_loss: 0.0901 - val_accuracy: 0.9691
    Epoch 7/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2284 - accuracy: 0.8833 - val_loss: 0.0899 - val_accuracy: 0.9676
    Epoch 8/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2317 - accuracy: 0.8865 - val_loss: 0.0812 - val_accuracy: 0.9762
    Epoch 9/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2310 - accuracy: 0.8839 - val_loss: 0.0784 - val_accuracy: 0.9769
    Epoch 10/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2260 - accuracy: 0.8834 - val_loss: 0.0808 - val_accuracy: 0.9762
    Epoch 11/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2307 - accuracy: 0.8841 - val_loss: 0.0725 - val_accuracy: 0.9791
    Epoch 12/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2253 - accuracy: 0.8848 - val_loss: 0.0796 - val_accuracy: 0.9733
    Epoch 13/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2254 - accuracy: 0.8847 - val_loss: 0.0756 - val_accuracy: 0.9768
    Epoch 14/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2202 - accuracy: 0.8879 - val_loss: 0.0790 - val_accuracy: 0.9762
    Epoch 15/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2244 - accuracy: 0.8830 - val_loss: 0.0817 - val_accuracy: 0.9773
    Epoch 16/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2173 - accuracy: 0.8873 - val_loss: 0.0880 - val_accuracy: 0.9762
    Epoch 17/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2244 - accuracy: 0.8842 - val_loss: 0.0749 - val_accuracy: 0.9807
    Epoch 18/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2162 - accuracy: 0.8897 - val_loss: 0.0662 - val_accuracy: 0.9767
    Epoch 19/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2233 - accuracy: 0.8858 - val_loss: 0.0801 - val_accuracy: 0.9777
    Epoch 20/20
    135/135 [==============================] - 1s 6ms/step - loss: 0.2200 - accuracy: 0.8879 - val_loss: 0.0702 - val_accuracy: 0.9811


With 98 percent I'm coming out the gate swinging. This is just simply looking at titles, not even looking at the inside of the text.

Before I continue, let me explain what each layer helps with.
Layering helps train the model and prevent overfitting. The vectorize layer will create a representation such that words with related meanings are close to each other in a vector space. Max tokens of the embedding layer will only track the top 2000 words. There were a few dropout layers to prevent overfitting. Global pooling helps reduce variance and computation complexity. These are just some layers I used to train this model.

Our next model will look at the text and base the guess off of that.

### Model 2


```python
text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```


```python
text_features = vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, 3, name = "textembedding")(text_features)
text_features = layers.Dropout(0.5)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.5)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
```


```python
main = layers.Dense(32, activation='relu')(text_features)
output = layers.Dense(2, name = "fake")(main)
```


```python
model2 = keras.Model(
    inputs = text_input,
    outputs = output
)
```


```python
model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model2.fit(train, 
                    validation_data=val,
                    epochs = 20)
```

    Epoch 1/20


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning:
    
    Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
    


    135/135 [==============================] - 2s 13ms/step - loss: 0.6567 - accuracy: 0.6455 - val_loss: 0.5307 - val_accuracy: 0.8362
    Epoch 2/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.3785 - accuracy: 0.8427 - val_loss: 0.1962 - val_accuracy: 0.9407
    Epoch 3/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2591 - accuracy: 0.8825 - val_loss: 0.1335 - val_accuracy: 0.9573
    Epoch 4/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2468 - accuracy: 0.8815 - val_loss: 0.1258 - val_accuracy: 0.9607
    Epoch 5/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2351 - accuracy: 0.8848 - val_loss: 0.1163 - val_accuracy: 0.9636
    Epoch 6/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2284 - accuracy: 0.8879 - val_loss: 0.0984 - val_accuracy: 0.9718
    Epoch 7/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2257 - accuracy: 0.8882 - val_loss: 0.0958 - val_accuracy: 0.9696
    Epoch 8/20
    135/135 [==============================] - 2s 12ms/step - loss: 0.2274 - accuracy: 0.8834 - val_loss: 0.0987 - val_accuracy: 0.9709
    Epoch 9/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2148 - accuracy: 0.8932 - val_loss: 0.0845 - val_accuracy: 0.9756
    Epoch 10/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2128 - accuracy: 0.8978 - val_loss: 0.0908 - val_accuracy: 0.9702
    Epoch 11/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2160 - accuracy: 0.8911 - val_loss: 0.0871 - val_accuracy: 0.9733
    Epoch 12/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2101 - accuracy: 0.8929 - val_loss: 0.0878 - val_accuracy: 0.9760
    Epoch 13/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2132 - accuracy: 0.8932 - val_loss: 0.0836 - val_accuracy: 0.9758
    Epoch 14/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2131 - accuracy: 0.8950 - val_loss: 0.0808 - val_accuracy: 0.9762
    Epoch 15/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2084 - accuracy: 0.8969 - val_loss: 0.0863 - val_accuracy: 0.9713
    Epoch 16/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2136 - accuracy: 0.8929 - val_loss: 0.0790 - val_accuracy: 0.9762
    Epoch 17/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2068 - accuracy: 0.8975 - val_loss: 0.0742 - val_accuracy: 0.9773
    Epoch 18/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2075 - accuracy: 0.8977 - val_loss: 0.0773 - val_accuracy: 0.9807
    Epoch 19/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2072 - accuracy: 0.8945 - val_loss: 0.0703 - val_accuracy: 0.9807
    Epoch 20/20
    135/135 [==============================] - 2s 11ms/step - loss: 0.2021 - accuracy: 0.8987 - val_loss: 0.0751 - val_accuracy: 0.9778


We used the same layers as before, since it worked well last time. Looking at our text yields a validation accuracy of about 98% as well. 

After running it, we saw that both models are doing well so far, let's see how well the model does when it looks at both the title and text at the same time.

### Model 3


```python
title_features = vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, 3, name = "titleembedding")(title_features)
title_features = layers.Dropout(0.5)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.5)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
```


```python
text_features = vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, 3, name = "textembedding")(text_features)
text_features = layers.Dropout(0.5)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.5)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
```


```python
main = layers.concatenate([title_features, text_features], axis = 1)
```


```python
main = layers.Dense(32, activation='relu')(main)
output = layers.Dense(2, name = "fake")(main)
```


```python
model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```


```python
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model3.fit(train, 
                    validation_data=val,
                    epochs = 20)
```

    Epoch 1/20
    135/135 [==============================] - 3s 14ms/step - loss: 0.6331 - accuracy: 0.6654 - val_loss: 0.4292 - val_accuracy: 0.9016
    Epoch 2/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.3037 - accuracy: 0.8854 - val_loss: 0.1297 - val_accuracy: 0.9611
    Epoch 3/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.1807 - accuracy: 0.9317 - val_loss: 0.0730 - val_accuracy: 0.9773
    Epoch 4/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.1393 - accuracy: 0.9474 - val_loss: 0.0447 - val_accuracy: 0.9885
    Epoch 5/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.1167 - accuracy: 0.9530 - val_loss: 0.0386 - val_accuracy: 0.9891
    Epoch 6/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.1035 - accuracy: 0.9602 - val_loss: 0.0293 - val_accuracy: 0.9909
    Epoch 7/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0968 - accuracy: 0.9608 - val_loss: 0.0254 - val_accuracy: 0.9916
    Epoch 8/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0962 - accuracy: 0.9592 - val_loss: 0.0293 - val_accuracy: 0.9920
    Epoch 9/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0922 - accuracy: 0.9616 - val_loss: 0.0241 - val_accuracy: 0.9918
    Epoch 10/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0902 - accuracy: 0.9633 - val_loss: 0.0139 - val_accuracy: 0.9958
    Epoch 11/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0827 - accuracy: 0.9674 - val_loss: 0.0158 - val_accuracy: 0.9944
    Epoch 12/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0886 - accuracy: 0.9661 - val_loss: 0.0175 - val_accuracy: 0.9951
    Epoch 13/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0834 - accuracy: 0.9656 - val_loss: 0.0157 - val_accuracy: 0.9957
    Epoch 14/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0808 - accuracy: 0.9689 - val_loss: 0.0145 - val_accuracy: 0.9962
    Epoch 15/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0775 - accuracy: 0.9687 - val_loss: 0.0103 - val_accuracy: 0.9973
    Epoch 16/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0808 - accuracy: 0.9683 - val_loss: 0.0162 - val_accuracy: 0.9951
    Epoch 17/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0813 - accuracy: 0.9665 - val_loss: 0.0166 - val_accuracy: 0.9947
    Epoch 18/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0854 - accuracy: 0.9646 - val_loss: 0.0135 - val_accuracy: 0.9960
    Epoch 19/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0840 - accuracy: 0.9637 - val_loss: 0.0163 - val_accuracy: 0.9944
    Epoch 20/20
    135/135 [==============================] - 2s 13ms/step - loss: 0.0832 - accuracy: 0.9660 - val_loss: 0.0137 - val_accuracy: 0.9962


Even with no modification between the two layers, combining the two models stabalizes around 99.5%. That is very, very good. Much better than humans, or else I don't think these articles would be so popular or a problem in the first place!

### Comparison

With all three models scoring above 98%, any would be a good choice to find fakenews. However, model 3 is the most accurate with a high val_accuracy of 99.8%. This suggests that using both title and text will best help determine whether an article is fake news or not.

## §4. Model Evaluation


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_data = pd.read_csv(test_url)
```


```python
test_dataset = make_dataset(test_data)
```


```python
model3.evaluate(test_dataset)
```

    22449/22449 [==============================] - 72s 3ms/step - loss: 0.0276 - accuracy: 0.9903





    [0.027611933648586273, 0.990333616733551]



## §5. Embedding Visualization


```python
weights = model3.get_layer('textembedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later
```


```python
weights
```




    array([[-0.00355777, -0.00924935,  0.00571733],
           [-0.01492204, -0.00057624,  0.01272109],
           [ 0.25367594,  0.2552968 , -0.3037704 ],
           ...,
           [-0.08817828, -0.06995765,  0.03057723],
           [-0.06141362, -0.05634517,  0.0900145 ],
           [ 0.11516449,  0.06687787, -0.13578564]], dtype=float32)




```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)
```


```python
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
embedding_df
```





  <div id="df-7aa58dce-bec6-4294-b618-a9fe833fa5b2">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>0.002333</td>
      <td>-0.002212</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>-0.003247</td>
      <td>0.008786</td>
    </tr>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>0.482909</td>
      <td>-0.028416</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trump</td>
      <td>-0.135477</td>
      <td>-0.010055</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>0.165011</td>
      <td>-0.040889</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>grounded</td>
      <td>-0.241641</td>
      <td>-0.009274</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>gross</td>
      <td>-0.172010</td>
      <td>0.000162</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>grilo</td>
      <td>-0.095112</td>
      <td>-0.028779</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>greatest</td>
      <td>-0.107312</td>
      <td>0.022824</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>governors</td>
      <td>0.196914</td>
      <td>-0.046146</td>
    </tr>
  </tbody>
</table>
<p>2000 rows × 3 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-7aa58dce-bec6-4294-b618-a9fe833fa5b2')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-7aa58dce-bec6-4294-b618-a9fe833fa5b2 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-7aa58dce-bec6-4294-b618-a9fe833fa5b2');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 20,
                 hover_name = "word")

fig.show()
```


{% include abc.html %}

To the right we have: "Trumps", "Militants", and "Clintons". Sounds like clickbait titles to me! Definitely fake news. 

To the left more neutral terms are used: "GOP", "Reportedly", "century". Usually those are harder to make juicy titles out of.

And just like that, you know how to detect fake news using Tensorflow!
