---
layout: post
title: Blog Post 5 - Image Classification
---

In this blog post, I will demonstrate how to perform image classification using Tensorflow, and will explain through demonstration concepts such as datasets, data augmentation, and transfer learning.

## ยง1. Load Packages and Obtain Data


Loading all the packages that we will use. Tensorflow will be our main package for our image classification.


```python
import os
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.python import keras
from tensorflow.python.keras import utils, layers, models, losses
from tensorflow.keras import utils
```

This next block of code will download all the pictures of dogs and cats and put them into useful datasets. These datasets will be used to train and test our model.


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
    68608000/68606236 [==============================] - 1s 0us/step
    68616192/68606236 [==============================] - 1s 0us/step
    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.


Applying tf.data AUTOUNE to our datasets will make it easier for them to by prefetching. "Prefetching overlaps the preprocessing and model excecution of a training set". https://www.tensorflow.org/guide/data_performance. In other words, it speeds up reading of the data.


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

### Working with Datasets

My inRows() function will create a plot that will sift through a data set and create two rows. The top rows will be cats, while the bottom rows will be dogs. 


```python
def inRows(dataset = train_dataset, columns = 3, batchSize = 1):

  #set indicies
  i = 0
  j = 0

  #for each image and its label in the training dataset
  for images, labels in train_dataset.take(batchSize):

    #makes a figure to plot images on
    plt.figure(figsize=(10, 10))

    #if i is less than the max number of columns wanted in the first row
    while i < columns:
      ax = plt.subplot(2, columns, i + 1)

      #plot a cat in the subplot
      if not labels[j].numpy():
        plt.imshow(images[j] / 255)
        plt.title('cat')
        plt.axis("off")
        i += 1
      j += 1

    #if i is more than the max number of columns wanted in the first row
    while i < 2*columns:
      ax = plt.subplot(2, columns, i + 1)

      #plot a dog in the subplot
      if labels[j].numpy():
        plt.imshow(images[j] / 255)
        plt.title('dog')
        plt.axis("off")
        i += 1
      j += 1
```


```python
inRows()
```

    /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
      # Remove the CWD from sys.path while we load stuff.
    /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.



    
![Figure of Cats on Top Row, Dogs on Bottom Row](/images/_output_8_1.png)


Cats on top and dogs on the bottom. Looks good to me.

### Check Label Frequencies

First we will iterate through the dataset, then we will count and store in a dictiionary the number of cat and dog pictures. 


```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```


```python
labels = [el for el in labels_iterator]
```


```python
dogCount = sum(1 for dog in labels if dog)
total = sum(1 for count in labels)
d = dict({"dogs": dogCount, "cats" : total-dogCount})
d
```




    {'cats': 1000, 'dogs': 1000}



This is exactly what we want to see. A baseline model always chooses the most frequent class. However, since there are two classes, and they are of equal size, our baseline model would be as accurate as a coin flip, because there are only two choices of equal probability. Any model we run from now on should be more accurate than 50% or it is worse than a random guess!

## ยง2. First Model

Our first model will rely solely on keras layers. I used a dropout of 2 to help protect against any overfitting. When I had no dropout layer, my validation accuracy was barely better than the baseline, so overfitting was most likely the cause of this. Eventually, it hit a point where **57% was the average validation accuracy**, which is ok with the paramters we gave. 


```python
model1 = models.Sequential([
    layers.Conv2D(13, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(13, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(2) # number of classes
])
```


```python
model1.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])
```


```python
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 17s 73ms/step - loss: 76.9614 - accuracy: 0.5275 - val_loss: 0.9079 - val_accuracy: 0.5198
    Epoch 2/20
    63/63 [==============================] - 5s 72ms/step - loss: 0.7617 - accuracy: 0.5930 - val_loss: 0.8183 - val_accuracy: 0.5334
    Epoch 3/20
    63/63 [==============================] - 5s 72ms/step - loss: 0.6316 - accuracy: 0.6640 - val_loss: 0.8757 - val_accuracy: 0.5248
    Epoch 4/20
    63/63 [==============================] - 5s 71ms/step - loss: 0.5646 - accuracy: 0.6995 - val_loss: 0.8850 - val_accuracy: 0.5557
    Epoch 5/20
    63/63 [==============================] - 5s 70ms/step - loss: 0.4566 - accuracy: 0.7670 - val_loss: 0.9883 - val_accuracy: 0.5421
    Epoch 6/20
    63/63 [==============================] - 5s 69ms/step - loss: 0.4022 - accuracy: 0.7970 - val_loss: 1.0462 - val_accuracy: 0.5483
    Epoch 7/20
    63/63 [==============================] - 5s 68ms/step - loss: 0.3530 - accuracy: 0.8335 - val_loss: 1.0894 - val_accuracy: 0.5569
    Epoch 8/20
    63/63 [==============================] - 5s 67ms/step - loss: 0.2962 - accuracy: 0.8705 - val_loss: 1.2467 - val_accuracy: 0.5384
    Epoch 9/20
    63/63 [==============================] - 4s 65ms/step - loss: 0.2544 - accuracy: 0.8870 - val_loss: 1.2825 - val_accuracy: 0.5532
    Epoch 10/20
    63/63 [==============================] - 4s 65ms/step - loss: 0.2303 - accuracy: 0.8980 - val_loss: 1.2960 - val_accuracy: 0.5495
    Epoch 11/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.2158 - accuracy: 0.9160 - val_loss: 1.4249 - val_accuracy: 0.5619
    Epoch 12/20
    63/63 [==============================] - 5s 67ms/step - loss: 0.2494 - accuracy: 0.9045 - val_loss: 1.3937 - val_accuracy: 0.5631
    Epoch 13/20
    63/63 [==============================] - 5s 68ms/step - loss: 0.2589 - accuracy: 0.9040 - val_loss: 1.5940 - val_accuracy: 0.5718
    Epoch 14/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.2113 - accuracy: 0.9120 - val_loss: 1.7015 - val_accuracy: 0.5705
    Epoch 15/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.2013 - accuracy: 0.9145 - val_loss: 1.9098 - val_accuracy: 0.5767
    Epoch 16/20
    63/63 [==============================] - 5s 68ms/step - loss: 0.1879 - accuracy: 0.9255 - val_loss: 1.9213 - val_accuracy: 0.5792
    Epoch 17/20
    63/63 [==============================] - 4s 66ms/step - loss: 0.1515 - accuracy: 0.9450 - val_loss: 2.1882 - val_accuracy: 0.5804
    Epoch 18/20
    63/63 [==============================] - 4s 67ms/step - loss: 0.1621 - accuracy: 0.9375 - val_loss: 2.3793 - val_accuracy: 0.5606
    Epoch 19/20
    63/63 [==============================] - 5s 69ms/step - loss: 0.1454 - accuracy: 0.9505 - val_loss: 2.3594 - val_accuracy: 0.5743
    Epoch 20/20
    63/63 [==============================] - 5s 67ms/step - loss: 0.1344 - accuracy: 0.9550 - val_loss: 2.3735 - val_accuracy: 0.5767


However, 57% is still not too good. Let's try something else

## ยง3. Model with Data Augmentation

Data augmentation will help the model learn invariant features, or features that do not change under other circumstances. This does not include warping, but rather think flipping and rotating an image. The dog will still look like a dog, just an upside down one. This will help train our model for pictures that are less obvious. 

The following code is used to randomly flip our images.


```python
flip = tf.keras.layers.RandomFlip()
```


```python
for image, _ in train_dataset.take(1):
  
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(6):
    if not i:
      ax = plt.subplot(2, 3, i + 1)
      plt.imshow(first_image / 255)
    else:
      ax = plt.subplot(2, 3, i + 1)
      augmented_image = flip(tf.expand_dims(first_image, 0))
      plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
 
```


    
![Figure of Cat flipping horizontally and vertically](/images/output_24_0.png)
    


The following code is used to randomly rotate our images.


```python
rotate = tf.keras.layers.RandomRotation((0,1))
```


```python
for image, _ in train_dataset.take(1):
  
  plt.figure(figsize=(10, 10))
  first_image = image[0]

  for i in range(6):
    ax = plt.subplot(2, 3, i + 1)
    if not i:
      plt.imshow(first_image / 255)
    else:
      augmented_image = rotate(tf.expand_dims(first_image, 0))
      plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![Figure of Dog doing a frontflip (not really his picture is just rotating)](/images/output_27_0.png)
    


Hes doin flips!

Now we will incorporate these data augmentations as layers in our model. Also, notice how the dropout rate has increased. This will help prevent overfitting and train a more accurate model.


```python
model2 = tf.keras.models.Sequential([
        flip,
        rotate,
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.5),
        layers.Flatten(),
        layers.Dense(2)
])
```


```python
model2.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])
```


```python
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 78ms/step - loss: 24.3224 - accuracy: 0.5170 - val_loss: 0.7097 - val_accuracy: 0.5223
    Epoch 2/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.7055 - accuracy: 0.5260 - val_loss: 0.7054 - val_accuracy: 0.5272
    Epoch 3/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.7023 - accuracy: 0.5180 - val_loss: 0.6961 - val_accuracy: 0.5309
    Epoch 4/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6943 - accuracy: 0.5345 - val_loss: 0.6985 - val_accuracy: 0.5210
    Epoch 5/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6962 - accuracy: 0.5265 - val_loss: 0.6912 - val_accuracy: 0.5223
    Epoch 6/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6908 - accuracy: 0.5160 - val_loss: 0.6889 - val_accuracy: 0.5260
    Epoch 7/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.6883 - accuracy: 0.5320 - val_loss: 0.6850 - val_accuracy: 0.5322
    Epoch 8/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.6880 - accuracy: 0.5230 - val_loss: 0.6782 - val_accuracy: 0.5606
    Epoch 9/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6891 - accuracy: 0.5350 - val_loss: 0.6925 - val_accuracy: 0.5433
    Epoch 10/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6896 - accuracy: 0.5420 - val_loss: 0.6890 - val_accuracy: 0.5421
    Epoch 11/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6861 - accuracy: 0.5305 - val_loss: 0.6827 - val_accuracy: 0.5408
    Epoch 12/20
    63/63 [==============================] - 5s 74ms/step - loss: 0.6868 - accuracy: 0.5350 - val_loss: 0.6778 - val_accuracy: 0.5557
    Epoch 13/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6962 - accuracy: 0.5285 - val_loss: 0.6958 - val_accuracy: 0.5025
    Epoch 14/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6845 - accuracy: 0.5325 - val_loss: 0.6955 - val_accuracy: 0.5149
    Epoch 15/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6922 - accuracy: 0.5260 - val_loss: 0.6849 - val_accuracy: 0.5458
    Epoch 16/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.6807 - accuracy: 0.5605 - val_loss: 0.6893 - val_accuracy: 0.5532
    Epoch 17/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.6904 - accuracy: 0.5335 - val_loss: 0.6909 - val_accuracy: 0.5173
    Epoch 18/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6894 - accuracy: 0.5470 - val_loss: 0.6809 - val_accuracy: 0.5470
    Epoch 19/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6837 - accuracy: 0.5440 - val_loss: 0.6668 - val_accuracy: 0.5557
    Epoch 20/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6948 - accuracy: 0.5055 - val_loss: 0.6937 - val_accuracy: 0.5074


It looks like we got a **high of 56% validation accurary, with it stabilizing to about an averge of 55**, just barely above the cutoff. However, the validation accuracy compared to the first one actually went down! My guess is that our accuracy decreased due to overfitting. I especially suspect this because our accuracy goes down after recieving the high. Doing some preprocessing will help increase our accuracy. Let's take a look at that now.

Now, lets try transforming the data to see if that will advance our model.

## ยง4. Data Preprocessing

Data preprocessing is the act of manipulating the data in order to get faster and more accurate results. In the followeing code I will change the RGB scaling from 0-255, to 0-1. This will allow for the model to spend more time training the data and less on having to adjust to the 0-255 scale.


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```


```python
model3 = tf.keras.models.Sequential([
        preprocessor,                                     
        flip,
        rotate,
        layers.Conv2D(13, (3, 3), activation='relu', input_shape=(160, 160, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(13, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.2),
        layers.Flatten(),
        layers.Dense(2)
])
```


```python
model3.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])
```


```python
history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 72ms/step - loss: 0.7146 - accuracy: 0.5080 - val_loss: 0.6658 - val_accuracy: 0.6101
    Epoch 2/20
    63/63 [==============================] - 5s 72ms/step - loss: 0.6787 - accuracy: 0.5610 - val_loss: 0.6326 - val_accuracy: 0.6238
    Epoch 3/20
    63/63 [==============================] - 5s 69ms/step - loss: 0.6580 - accuracy: 0.5890 - val_loss: 0.6269 - val_accuracy: 0.6559
    Epoch 4/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.6403 - accuracy: 0.6190 - val_loss: 0.6109 - val_accuracy: 0.6473
    Epoch 5/20
    63/63 [==============================] - 5s 69ms/step - loss: 0.6433 - accuracy: 0.6190 - val_loss: 0.6148 - val_accuracy: 0.6597
    Epoch 6/20
    63/63 [==============================] - 5s 74ms/step - loss: 0.6114 - accuracy: 0.6575 - val_loss: 0.5899 - val_accuracy: 0.6844
    Epoch 7/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6144 - accuracy: 0.6515 - val_loss: 0.5930 - val_accuracy: 0.6597
    Epoch 8/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6076 - accuracy: 0.6625 - val_loss: 0.5964 - val_accuracy: 0.6683
    Epoch 9/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.5963 - accuracy: 0.6870 - val_loss: 0.5816 - val_accuracy: 0.6807
    Epoch 10/20
    63/63 [==============================] - 5s 72ms/step - loss: 0.6024 - accuracy: 0.6735 - val_loss: 0.5919 - val_accuracy: 0.6856
    Epoch 11/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.5930 - accuracy: 0.6810 - val_loss: 0.5719 - val_accuracy: 0.6894
    Epoch 12/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.5863 - accuracy: 0.6935 - val_loss: 0.5699 - val_accuracy: 0.6955
    Epoch 13/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.5785 - accuracy: 0.6890 - val_loss: 0.5676 - val_accuracy: 0.6968
    Epoch 14/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5850 - accuracy: 0.6905 - val_loss: 0.5681 - val_accuracy: 0.6782
    Epoch 15/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.5756 - accuracy: 0.6905 - val_loss: 0.5614 - val_accuracy: 0.6931
    Epoch 16/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.5721 - accuracy: 0.6995 - val_loss: 0.5528 - val_accuracy: 0.7129
    Epoch 17/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5677 - accuracy: 0.7090 - val_loss: 0.5596 - val_accuracy: 0.6980
    Epoch 18/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.5646 - accuracy: 0.6985 - val_loss: 0.5710 - val_accuracy: 0.7005
    Epoch 19/20
    63/63 [==============================] - 6s 97ms/step - loss: 0.5673 - accuracy: 0.6995 - val_loss: 0.5692 - val_accuracy: 0.6993
    Epoch 20/20
    63/63 [==============================] - 8s 124ms/step - loss: 0.5632 - accuracy: 0.7090 - val_loss: 0.5377 - val_accuracy: 0.7290


As you can see, it **stabalized around 70%**, which is much better than before. We even had a high or 72.9%! Additionally, we could decrease the dropout value, as this model did not have as much trouble with overfitting than our other model did.

Now we can try one last addition to really increase our model accuracy

## ยง5. Transfer Learning

As you might have guessed, this isn't the accurate model for distinguishing between cats and dogs. However, someone out there has already created a model much better than ours. Using the following code, we can preload it, insert it as a layer into our model, and use it as a baseline. It can only get better from here!... right?


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```


```python
model4 = tf.keras.models.Sequential([
        preprocessor,                                     
        flip,
        rotate,
        base_model_layer,
        layers.Flatten(),
        layers.Dense(2)
])
```


```python
model4.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])
```


```python
history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 12s 112ms/step - loss: 0.7607 - accuracy: 0.8785 - val_loss: 0.1790 - val_accuracy: 0.9653
    Epoch 2/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.6557 - accuracy: 0.9060 - val_loss: 0.3120 - val_accuracy: 0.9641
    Epoch 3/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.7036 - accuracy: 0.9220 - val_loss: 0.2701 - val_accuracy: 0.9641
    Epoch 4/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.7890 - accuracy: 0.9100 - val_loss: 0.3010 - val_accuracy: 0.9641
    Epoch 5/20
    63/63 [==============================] - 7s 101ms/step - loss: 0.8400 - accuracy: 0.9210 - val_loss: 0.2754 - val_accuracy: 0.9715
    Epoch 6/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.5983 - accuracy: 0.9365 - val_loss: 0.3164 - val_accuracy: 0.9629
    Epoch 7/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.6001 - accuracy: 0.9335 - val_loss: 0.5441 - val_accuracy: 0.9493
    Epoch 8/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.7442 - accuracy: 0.9310 - val_loss: 0.3463 - val_accuracy: 0.9653
    Epoch 9/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.5350 - accuracy: 0.9400 - val_loss: 0.3440 - val_accuracy: 0.9678
    Epoch 10/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.4338 - accuracy: 0.9545 - val_loss: 0.4560 - val_accuracy: 0.9579
    Epoch 11/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.5950 - accuracy: 0.9390 - val_loss: 0.4283 - val_accuracy: 0.9567
    Epoch 12/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.5289 - accuracy: 0.9470 - val_loss: 0.4217 - val_accuracy: 0.9579
    Epoch 13/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.5749 - accuracy: 0.9355 - val_loss: 0.5296 - val_accuracy: 0.9592
    Epoch 14/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.4142 - accuracy: 0.9550 - val_loss: 0.4048 - val_accuracy: 0.9567
    Epoch 15/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.7092 - accuracy: 0.9420 - val_loss: 0.5325 - val_accuracy: 0.9616
    Epoch 16/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.5181 - accuracy: 0.9505 - val_loss: 0.5815 - val_accuracy: 0.9554
    Epoch 17/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.7293 - accuracy: 0.9340 - val_loss: 0.4035 - val_accuracy: 0.9629
    Epoch 18/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.5075 - accuracy: 0.9550 - val_loss: 0.5286 - val_accuracy: 0.9567
    Epoch 19/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.5874 - accuracy: 0.9505 - val_loss: 0.5111 - val_accuracy: 0.9530
    Epoch 20/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.6285 - accuracy: 0.9410 - val_loss: 0.4618 - val_accuracy: 0.9616


Wow! It did get alot better. We didn't even have to add any additional layers like dropout and pooling that we had earlier. In the few iterations I experimented with, I did not find them to be a significant help in increasing the value, thought **it stabalizing around 96%** is nothing to be sad about. Again, I think overfitting may have prevented it from goin even higher, becauase our highest epoch was the fifth one, and it did not increase after 15 iterations. However, in comparison to the other model, this obviously was our most succesful version.

## ยง6. Score on Test Data

Below you can see that our test accuracy came out to essentially 98%. That means for ever 100 pictures of dogs and cats, only 2 are incorrect. I bet there's some humans with the same accuracy, so I'd say this model did pretty good.


```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```

    6/6 [==============================] - 1s 65ms/step - loss: 0.3652 - accuracy: 0.9792
    Test accuracy : 0.9791666865348816


### And just like that, you now know how to train an image classification model. 
